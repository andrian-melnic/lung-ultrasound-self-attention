{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["'/Users/andry/Documents/GitHub/lus-dl-framework'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["working_dir = \"../.\"\n","dataset_h5_path = \"/Users/andry/Documents/GitHub/lus-dl-framework/data/iclus/dataset.h5\"\n","hospitaldict_path = \"/Users/andry/Documents/GitHub/lus-dl-framework/data/iclus/hospitals-patients-dict.pkl\"\n","libraries_dir = working_dir + \"/libraries\"\n","\n","\n","import warnings\n","import sys\n","import os\n","import glob\n","import pickle\n","import lightning as pl\n","from tabulate import tabulate\n","from torch.utils.data import Subset\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","from kornia import tensor_to_image\n","from transformers import ViTForImageClassification\n","from transformers import ViTImageProcessor\n","from transformers import ViTConfig\n","\n","\n","from lightning.pytorch.loggers import TensorBoardLogger\n","from lightning.pytorch import Trainer\n","from lightning.pytorch.callbacks import EarlyStopping, DeviceStatsMonitor, ModelCheckpoint\n","from lightning.pytorch.tuner import Tuner\n","\n","\n","sys.path.append(working_dir)\n","from data_setup import HDF5Dataset, FrameTargetDataset, DataAugmentation\n","\n","os.chdir(working_dir)\n","os.getcwd()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import pickle\n","from torch.utils.data import Subset\n","\n","train_ratio = 0.2\n","rseed = 21"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Serialized frame index map FOUND.\n","\n","Loaded serialized data.\n","\n","\n","277 videos (58924 frames) loaded.\n"]}],"source":["dataset = HDF5Dataset(dataset_h5_path)\n","\n","train_indices_path = os.path.dirname(dataset_h5_path) + f\"/train_indices_{train_ratio}.pkl\"\n","test_indices_path = os.path.dirname(dataset_h5_path) + f\"/test_indices_{train_ratio}.pkl\""]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pickled indices\n"]}],"source":["if os.path.exists(train_indices_path) and os.path.exists(test_indices_path):\n","    print(\"Loading pickled indices\")\n","    with open(train_indices_path, 'rb') as train_pickle_file:\n","        train_indices = pickle.load(train_pickle_file)\n","    with open(test_indices_path, 'rb') as test_pickle_file:\n","        test_indices = pickle.load(test_pickle_file)\n","    # Create training and test subsets\n","    train_subset = Subset(dataset, train_indices)\n","    test_subset = Subset(dataset, test_indices)  \n","else:\n","    train_subset, test_subset, split_info, train_indices, test_indices = dataset.split_dataset(hospitaldict_path, \n","                                                              rseed, \n","                                                              train_ratio)\n","    print(\"Pickling sets...\")\n","    \n","    # Pickle the indices\n","    with open(train_indices_path, 'wb') as train_pickle_file:\n","        pickle.dump(train_indices, train_pickle_file)\n","    with open(test_indices_path, 'wb') as test_pickle_file:\n","        pickle.dump(test_indices, test_pickle_file)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["<torch.utils.data.dataset.Subset at 0x1744d0710>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["test_subset_size = train_ratio/2\n","test_subset = Subset(test_subset, range(int(test_subset_size * len(test_indices))))\n","test_subset"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train size: 8052\n","Test size: 5087\n"]}],"source":["train_dataset = FrameTargetDataset(train_subset)\n","test_dataset = FrameTargetDataset(test_subset)\n","\n","print(f\"Train size: {len(train_dataset)}\")\n","print(f\"Test size: {len(test_dataset)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Dataloader"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def collate_fn(examples):\n","    frames = torch.stack([example[0] for example in examples])  # Extract the preprocessed frames\n","    scores = torch.tensor([example[1] for example in examples])  # Extract the scores\n","    return {\"pixel_values\": frames, \"labels\": scores}\n","    # return (frames, scores)\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["preprocess = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k', do_rescale=False)\n","transform = DataAugmentation()\n","\n","# train_dataset.set_transform(preprocess)\n","# test_dataset.set_transform(preprocess)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["batch_size = 16\n","train_dataloader = DataLoader(train_dataset,\n","                          batch_size=batch_size,\n","                          num_workers=0,\n","                          pin_memory=True,\n","                          collate_fn=collate_fn, shuffle=False)\n","\n","test_dataloader = DataLoader(test_dataset,\n","                          batch_size=batch_size,\n","                          pin_memory=True,\n","                          collate_fn=collate_fn)\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'str' object has no attribute 'shape'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb Cella 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         plt\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# To display one batch from the training DataLoader\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X30sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m show_batch(train_dataloader, num_batches\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n","\u001b[1;32m/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb Cella 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Apply data augmentation to the batch (you need to define DataAugmentation function)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m imgs_aug \u001b[39m=\u001b[39m transform(imgs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Create subplots for original and augmented images\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39mwin_size)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/Documents/GitHub/lus-dl-framework/notebooks/.././data_setup.py:312\u001b[0m, in \u001b[0;36mDataAugmentation.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()  \u001b[39m# disable gradients for efficiency\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    304\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform data augmentation on input tensor.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39m        torch.Tensor: Augmented tensor of shape BxCxHxW.\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m     x_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransforms(x)\n\u001b[1;32m    313\u001b[0m     \u001b[39mreturn\u001b[39;00m x_out\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/kornia/augmentation/base.py:199\u001b[0m, in \u001b[0;36m_BasicAugmentationBase.forward\u001b[0;34m(self, input, params, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Perform forward operations.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \n\u001b[1;32m    187\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39m    ``save_kwargs=True`` additionally.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m in_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__unpack_input__(\u001b[39minput\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m input_shape \u001b[39m=\u001b[39m in_tensor\u001b[39m.\u001b[39;49mshape\n\u001b[1;32m    200\u001b[0m in_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_tensor(in_tensor)\n\u001b[1;32m    201\u001b[0m batch_shape \u001b[39m=\u001b[39m in_tensor\u001b[39m.\u001b[39mshape\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"]}],"source":["\n","def show_batch(data_loader, num_batches=1, win_size=(20, 20)):\n","    def _to_vis(data):\n","        # Ensure that pixel values are in the valid range [0, 1]\n","        data = torch.clamp(data, 0, 1)\n","        return tensor_to_image(torchvision.utils.make_grid(data, nrow=8))\n","\n","    for batch_num, (imgs, labels) in enumerate(data_loader):\n","        if batch_num >= num_batches:\n","            break\n","        \n","        # Apply data augmentation to the batch (you need to define DataAugmentation function)\n","        imgs_aug = transform(imgs)\n","\n","        # Create subplots for original and augmented images\n","        plt.figure(figsize=win_size)\n","        plt.subplot(1, 2, 1)\n","        plt.imshow(_to_vis(imgs))\n","        plt.title(\"Original Images\")\n","\n","        plt.subplot(1, 2, 2)\n","        plt.imshow(_to_vis(imgs_aug))\n","        plt.title(\"Augmented Images\")\n","\n","        plt.show()\n","\n","# To display one batch from the training DataLoader\n","show_batch(train_dataloader, num_batches=3)"]},{"cell_type":"markdown","metadata":{},"source":["# Models"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n","- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n","- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["pretrained = True\n","\n","configuration = {\n","    \"num_labels\": 4,\n","    \"num_attention_heads\": 4,\n","    \"num_hidden_layers\":4\n","}\n","\n","id2label = {0: 'no', 1: 'yellow', 2: 'orange', 3: 'red'}\n","label2id = {\"no\": 0, \"yellow\": 1, \"orange\": 2, \"red\": 3}\n","\n","if pretrained == True:\n","    vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\",\n","                                                            num_labels=4,\n","                                                            id2label=id2label,\n","                                                            label2id=label2id,\n","                                                            ignore_mismatched_sizes = True)\n","else:\n","    config = ViTConfig(**configuration)\n","    vit = ViTForImageClassification(config=config)"]},{"cell_type":"markdown","metadata":{},"source":["# Model configuration"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["â•’â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â••\n","â”‚               â”‚ MODEL HYPERPARAMETERS                                               â”‚\n","â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n","â”‚ model         â”‚ google_vit                                                          â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ batch_size    â”‚ 16                                                                  â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lr            â”‚ 0.0001                                                              â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ optimizer     â”‚ sgd                                                                 â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ num_workers   â”‚ 0                                                                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ pretrained    â”‚ False                                                               â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ configuration â”‚ {'num_labels': 4, 'num_attention_heads': 4, 'num_hidden_layers': 4} â”‚\n","â•˜â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•›\n"]}],"source":["selected_model=\"google_vit\"\n","\n","hyperparameters = {\n","  \"train_dataset\": train_dataset,\n","  \"test_dataset\": test_dataset,\n","  \"batch_size\": 16,\n","  \"lr\": 0.0001,\n","  \"optimizer\": \"sgd\",\n","  \"num_workers\": 0,\n","  \"pretrained\": False,\n","  \"configuration\": configuration\n","}\n","# Instantiate lightning model\n","if selected_model == \"google_vit\":\n","  model = vit\n","elif selected_model == \"resnet18\":\n","  model =  ResNet18LightningModule(**hyperparameters)\n","elif selected_model == \"beit\": \n","  model =  BEiTLightningModule(**hyperparameters)\n","else:\n","  raise ValueError(\"Invalid model name. Please choose either 'google_vit' or 'resnet18'.\")\n","\n","table_data = []\n","table_data.append([\"MODEL HYPERPARAMETERS\"])\n","table_data.append([\"model\", selected_model])\n","for key, value in hyperparameters.items():\n","    if key not in [\"train_dataset\", \"test_dataset\"]:\n","      table_data.append([key, value])\n","\n","table = tabulate(table_data, headers=\"firstrow\", tablefmt=\"fancy_grid\")\n","print(table)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["ViTConfig {\n","  \"_name_or_path\": \"google/vit-base-patch16-224\",\n","  \"architectures\": [\n","    \"ViTForImageClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"no\",\n","    \"1\": \"yellow\",\n","    \"2\": \"orange\",\n","    \"3\": \"red\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"no\": 0,\n","    \"orange\": 2,\n","    \"red\": 3,\n","    \"yellow\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"qkv_bias\": true,\n","  \"transformers_version\": \"4.33.3\"\n","}"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["model.config"]},{"cell_type":"markdown","metadata":{},"source":["# Trainer configuration"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","import numpy as np\n","\n","# Logger configuration\n","name_trained = \"pretrained_\" if pretrained==True else \"\"\n","model_name = f\"{name_trained}{selected_model}/{hyperparameters['optimizer']}/{hyperparameters['lr']}_{hyperparameters['batch_size']}\"\n","logger = TensorBoardLogger(\"tb_logs\", name=model_name)\n","\n","args = TrainingArguments(\n","    model_name,\n","    remove_unused_columns=False,\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=hyperparameters['lr'],\n","    per_device_train_batch_size=hyperparameters['batch_size'],\n","    gradient_accumulation_steps=4,\n","    per_device_eval_batch_size=hyperparameters['batch_size'],\n","    num_train_epochs=3,\n","    optim=hyperparameters['optimizer'],\n","    warmup_ratio=0.1,\n","    logging_steps=10,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/5n/6lbgb72n3qdfxvx15vxspc3c0000gn/T/ipykernel_72281/1746353928.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"accuracy\")\n"]}],"source":["from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\")\n","# the compute_metrics function takes a Named Tuple as input:\n","# predictions, which are the logits of the model as Numpy arrays,\n","# and label_ids, which are the ground-truth labels as Numpy arrays.\n","def compute_metrics(eval_pred):\n","    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n","    predictions = np.argmax(eval_pred.predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n","    data_collator=collate_fn,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7426b0aa65ad47539bef307904f33355","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/378 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/Users/andry/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb Cella 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_results \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# rest is optional but nice to have\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/pytorch_testing.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model()\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/transformers/trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1555\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1556\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1557\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1558\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1559\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1560\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1561\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/transformers/trainer.py:1843\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1837\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m   1838\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1840\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1841\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1842\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1843\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39;49misinf(tr_loss_step))\n\u001b[1;32m   1844\u001b[0m ):\n\u001b[1;32m   1845\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1846\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1847\u001b[0m \u001b[39melse\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_results = trainer.train()\n","# rest is optional but nice to have\n","trainer.save_model()\n","trainer.log_metrics(\"train\", train_results.metrics)\n","trainer.save_metrics(\"train\", train_results.metrics)\n","trainer.save_state()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
