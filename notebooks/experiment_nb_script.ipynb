{"cells":[{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["'/Users/andry/Documents'"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["working_dir = \"../.\"\n","dataset_h5_path = \"/Users/andry/Documents/GitHub/lus-dl-framework/data/iclus/dataset.h5\"\n","hospitaldict_path = \"/Users/andry/Documents/GitHub/lus-dl-framework/data/iclus/hospitals-patients-dict.pkl\"\n","libraries_dir = working_dir + \"/libraries\"\n","\n","\n","import warnings\n","import sys\n","import os\n","import glob\n","import pickle\n","import lightning as pl\n","from tabulate import tabulate\n","from torch.utils.data import Subset\n","\n","from lightning.pytorch.loggers import TensorBoardLogger\n","from lightning.pytorch import Trainer\n","from lightning.pytorch.callbacks import EarlyStopping, DeviceStatsMonitor, ModelCheckpoint\n","from lightning.pytorch.tuner import Tuner\n","\n","\n","sys.path.append(working_dir)\n","from data_setup import HDF5Dataset, FrameTargetDataset\n","from lightning_modules.ViTLightningModule import ViTLightningModule\n","from lightning_modules.ResNet18LightningModule import ResNet18LightningModule\n","from lightning_modules.BEiTLightningModule import BEiTLightningModule\n","\n","os.chdir(working_dir)\n","os.getcwd()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import pickle\n","from torch.utils.data import Subset\n","\n","train_ratio = 0.2\n","rseed = 21"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Serialized frame index map FOUND.\n","\n","Loaded serialized data.\n","\n","\n","277 videos (58924 frames) loaded.\n"]}],"source":["dataset = HDF5Dataset(dataset_h5_path)\n","\n","train_indices_path = os.path.dirname(dataset_h5_path) + f\"/train_indices_{train_ratio}.pkl\"\n","test_indices_path = os.path.dirname(dataset_h5_path) + f\"/test_indices_{train_ratio}.pkl\""]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading pickled indices\n"]}],"source":["if os.path.exists(train_indices_path) and os.path.exists(test_indices_path):\n","    print(\"Loading pickled indices\")\n","    with open(train_indices_path, 'rb') as train_pickle_file:\n","        train_indices = pickle.load(train_pickle_file)\n","    with open(test_indices_path, 'rb') as test_pickle_file:\n","        test_indices = pickle.load(test_pickle_file)\n","    # Create training and test subsets\n","    train_subset = Subset(dataset, train_indices)\n","    test_subset = Subset(dataset, test_indices)  \n","else:\n","    train_subset, test_subset, split_info, train_indices, test_indices = dataset.split_dataset(hospitaldict_path, \n","                                                              rseed, \n","                                                              train_ratio)\n","    print(\"Pickling sets...\")\n","    \n","    # Pickle the indices\n","    with open(train_indices_path, 'wb') as train_pickle_file:\n","        pickle.dump(train_indices, train_pickle_file)\n","    with open(test_indices_path, 'wb') as test_pickle_file:\n","        pickle.dump(test_indices, test_pickle_file)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["<torch.utils.data.dataset.Subset at 0x17f313610>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["test_subset_size = train_ratio/2\n","test_subset = Subset(test_subset, range(int(test_subset_size * len(test_indices))))\n","test_subset"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train size: 11053\n","Test size: 4787\n"]}],"source":["train_dataset = FrameTargetDataset(train_subset)\n","test_dataset = FrameTargetDataset(test_subset)\n","\n","print(f\"Train size: {len(train_dataset)}\")\n","print(f\"Test size: {len(test_dataset)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Models"]},{"cell_type":"markdown","metadata":{},"source":["## ViT"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# Model class ------------------------------------------------------------\n","from transformers import ViTForImageClassification\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import lightning.pytorch as pl\n","import torch\n","import torchvision\n","from transformers import ViTImageProcessor\n","from torchmetrics.classification import MulticlassF1Score\n","from kornia import tensor_to_image\n","import matplotlib.pyplot as plt\n","from data_setup import DataAugmentation\n","from transformers import ViTConfig\n","\n","\n","def collate_fn(examples):\n","    frames = torch.stack([example[0] for example in examples])  # Extract the preprocessed frames\n","    scores = torch.tensor([example[1] for example in examples])  # Extract the scores\n","    return (frames, scores)\n","  \n","id2label = {0: 'no', 1: 'yellow', 2: 'orange', 3: 'red'}\n","label2id = {\"no\": 0, \"yellow\": 1, \"orange\": 2, \"red\": 3}\n","\n","class ViTLightningModule(pl.LightningModule):\n","    def __init__(self, \n","                 train_dataset,\n","                 test_dataset,\n","                 batch_size,\n","                 num_workers,\n","                 optimizer,\n","                 num_classes=4,\n","                 lr=1e-3,\n","                 pretrained=True,\n","                 configuration=None):\n","        \n","        super(ViTLightningModule, self).__init__()\n","        \n","        if pretrained == True:\n","            self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n","                                                                  num_labels=4,\n","                                                                  id2label=id2label,\n","                                                                  label2id=label2id)\n","        else:\n","            self.config = ViTConfig(**configuration)\n","            self.vit = ViTForImageClassification(config=self.config)\n","        \n","        self.preprocess = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k', do_rescale=False)\n","        self.transform = DataAugmentation()\n","        \n","        self.train_dataset = train_dataset\n","        self.train_dataset.set_transform(self.preprocess)\n","        self.test_dataset = test_dataset\n","        self.test_dataset.set_transform(self.preprocess)\n","        \n","        \n","        self.num_classes = num_classes\n","        self.lr = lr\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\\\n","        self.optimizer_name = str(optimizer).lower()\n","        self.optimizer = None\n","        self.f1_score_metric = MulticlassF1Score(num_classes=num_classes)\n","        \n","        \n","    # def on_after_batch_transfer(self, batch, dataloader_idx):\n","    #     pixel_values, labels = batch\n","    #     if self.trainer.training:\n","    #         x = self.transform(pixel_values)  # => we perform GPU/Batched data augmentation\n","    #     return x, labels\n","      \n","      \n","    def forward(self, pixel_values):\n","        outputs = self.vit(pixel_values=pixel_values)\n","        return outputs.logits\n","      \n","    \n","    def common_step(self, batch, batch_idx):\n","      \n","        pixel_values, labels = batch\n","        \n","        logits = self(pixel_values)\n","\n","        criterion = nn.CrossEntropyLoss()\n","        loss = criterion(logits, labels)\n","        predictions = logits.argmax(-1)\n","        correct = (predictions == labels).sum().item()\n","        accuracy = correct/pixel_values.shape[0]\n","        #accuracy = torchmetrics.functional.accuracy(predictions, labels, task=\"multiclass\", num_classes=4)\n","        f1 = self.f1_score_metric(logits, labels)\n","\n","        return loss, accuracy, f1\n","      \n","    def training_step(self, batch, batch_idx):\n","        loss, accuracy, f1 = self.common_step(batch, batch_idx)\n","        self.log(\"training_loss\", loss, on_epoch=True, prog_bar=True)\n","        self.log(\"training_accuracy\", accuracy, on_epoch=True, prog_bar=True)\n","        self.log(\"training_f1\", f1, on_epoch=True, prog_bar=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss, accuracy, f1 = self.common_step(batch, batch_idx)\n","        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n","        self.log(\"test_accuracy\", accuracy, on_epoch=True, prog_bar=True)\n","        self.log(\"test_f1\", f1, on_epoch=True, prog_bar=True)\n","\n","        return loss\n","\n","    def configure_optimizers(self):\n","        if self.optimizer_name == \"adam\":\n","            self.optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=0.05)\n","        elif self.optimizer_name == \"sgd\":\n","            self.optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n","        else:\n","            raise ValueError(\"Invalid optimizer name. Please choose either 'adam' or 'sgd'.\")\n","\n","        return self.optimizer\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset,\n","                          batch_size=self.batch_size,\n","                          num_workers=self.num_workers,\n","                          pin_memory=True,\n","                          collate_fn=collate_fn, shuffle=False)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.test_dataset,\n","                          batch_size=self.batch_size,\n","                          pin_memory=True,\n","                          collate_fn=collate_fn)\n","\n","\n","    def show_batch(self, win_size=(10, 10)):\n","        def _to_vis(data):\n","            # Ensure that pixel values are in the valid range [0, 1]\n","            data = torch.clamp(data, 0, 1)\n","            return tensor_to_image(torchvision.utils.make_grid(data, nrow=8))\n","\n","        # Get a batch from the training set\n","        imgs, labels = next(iter(self.train_dataloader()))\n","\n","        # Apply data augmentation to the batch\n","        imgs_aug = self.transform(imgs)\n","\n","        # Use matplotlib to visualize the original and augmented images\n","        plt.figure(figsize=win_size)\n","        plt.imshow(_to_vis(imgs))\n","        plt.title(\"Original Images\")\n","\n","        plt.figure(figsize=win_size)\n","        plt.imshow(_to_vis(imgs_aug))\n","        plt.title(\"Augmented Images\")"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{},"source":["## Model configuration"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["╒═══════════════╤═════════════════════════════════════════════════════════════════════╕\n","│               │ MODEL HYPERPARAMETERS                                               │\n","╞═══════════════╪═════════════════════════════════════════════════════════════════════╡\n","│ model         │ google_vit                                                          │\n","├───────────────┼─────────────────────────────────────────────────────────────────────┤\n","│ batch_size    │ 4                                                                   │\n","├───────────────┼─────────────────────────────────────────────────────────────────────┤\n","│ lr            │ 0.0001                                                              │\n","├───────────────┼─────────────────────────────────────────────────────────────────────┤\n","│ optimizer     │ sgd                                                                 │\n","├───────────────┼─────────────────────────────────────────────────────────────────────┤\n","│ num_workers   │ 0                                                                   │\n","├───────────────┼─────────────────────────────────────────────────────────────────────┤\n","│ pretrained    │ True                                                                │\n","├───────────────┼─────────────────────────────────────────────────────────────────────┤\n","│ configuration │ {'num_labels': 4, 'num_attention_heads': 4, 'num_hidden_layers': 4} │\n","╘═══════════════╧═════════════════════════════════════════════════════════════════════╛\n"]}],"source":["selected_model=\"google_vit\"\n","\n","\n","configuration = {\n","    \"num_labels\": 4,\n","    \"num_attention_heads\": 4,\n","    \"num_hidden_layers\":4\n","}\n","\n","hyperparameters = {\n","  \"train_dataset\": train_dataset,\n","  \"test_dataset\": test_dataset,\n","  \"batch_size\": 4,\n","  \"lr\": 0.0001,\n","  \"optimizer\": \"sgd\",\n","  \"num_workers\": 0,\n","  \"pretrained\": True,\n","  \"configuration\": configuration\n","}\n","# Instantiate lightning model\n","if selected_model == \"google_vit\":\n","  model = ViTLightningModule(**hyperparameters)\n","elif selected_model == \"resnet18\":\n","  model =  ResNet18LightningModule(**hyperparameters)\n","elif selected_model == \"beit\": \n","  model =  BEiTLightningModule(**hyperparameters)\n","else:\n","  raise ValueError(\"Invalid model name. Please choose either 'google_vit' or 'resnet18'.\")\n","\n","table_data = []\n","table_data.append([\"MODEL HYPERPARAMETERS\"])\n","table_data.append([\"model\", selected_model])\n","for key, value in hyperparameters.items():\n","    if key not in [\"train_dataset\", \"test_dataset\"]:\n","      table_data.append([key, value])\n","\n","table = tabulate(table_data, headers=\"firstrow\", tablefmt=\"fancy_grid\")\n","print(table)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["ViTConfig {\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 4,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 4,\n","  \"patch_size\": 16,\n","  \"qkv_bias\": true,\n","  \"transformers_version\": \"4.33.3\"\n","}"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["model.config"]},{"cell_type":"markdown","metadata":{},"source":["## Trainer configuration"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["# Logger configuration\n","name_trained = \"pretrained_\" if hyperparameters[\"pretrained\"]==True else \"\"\n","model_name = f\"{name_trained}{selected_model}/{hyperparameters['optimizer']}/{hyperparameters['lr']}_{hyperparameters['batch_size']}\"\n","logger = TensorBoardLogger(\"tb_logs\", name=model_name)\n","\n","callbacks = []\n","\n","checkpoint_dir = f\"{working_dir}/checkpoints/{model_name}\"\n","checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_dir, \n","                                      save_top_k=3,\n","                                      mode=\"min\",\n","                                      monitor=\"training_loss\",\n","                                      save_last=True,\n","                                      verbose=True)\n","callbacks.append(checkpoint_callback)\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/andry/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n","  rank_zero_warn(\n","Using 16bit Automatic Mixed Precision (AMP)\n","/Users/andry/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\n","GPU available: True (mps), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","╒═════════════════════════╤═════════════════════╕\n","│                         │ TRAINER ARGUMENTS   │\n","╞═════════════════════════╪═════════════════════╡\n","│ accelerator             │ mps                 │\n","├─────────────────────────┼─────────────────────┤\n","│ max_epochs              │ 5                   │\n","├─────────────────────────┼─────────────────────┤\n","│ precision               │ 16                  │\n","├─────────────────────────┼─────────────────────┤\n","│ accumulate_grad_batches │ 16                  │\n","╘═════════════════════════╧═════════════════════╛\n","Model checkpoints directory is .././checkpoints/pretrained_google_vit/sgd/0.0001_4\n","\n","\n","\n","\n","\n","--------------------\n","Trainer Callbacks:\n","--------------------\n","\n","\n","- TQDMProgressBar\n","- ModelSummary\n","- ModelCheckpoint\n"]}],"source":["\n","trainer_args = {\n","    \"accelerator\": \"mps\",\n","    \"max_epochs\": 5,\n","    \"callbacks\": callbacks,\n","    \"precision\": 16,\n","    \"accumulate_grad_batches\": 16,\n","    \"logger\": logger\n","}\n","table_data = []\n","table_data.append([\"TRAINER ARGUMENTS\"])\n","for key, value in trainer_args.items():\n","    if key not in [\"callbacks\", \"logger\"]:\n","        table_data.append([key, value])\n","\n","table = tabulate(table_data, headers=\"firstrow\", tablefmt=\"fancy_grid\")\n","print(\"\\n\\n\" + table)\n","print(f\"Model checkpoints directory is {checkpoint_dir}\")\n","print(\"\\n\\n\")\n","trainer = Trainer(**trainer_args,\n","                  default_root_dir = checkpoint_dir)\n","\n","print(\"\\n\\n\" + \"-\" * 20)\n","print(\"Trainer Callbacks:\")\n","print(\"-\" * 20 + \"\\n\\n\")\n","for callback in trainer.callbacks:\n","    print(f\"- {type(callback).__name__}\")"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Instantiating trainer without checkpoint...\n"]},{"name":"stderr","output_type":"stream","text":["\n","  | Name            | Type                      | Params\n","--------------------------------------------------------------\n","0 | vit             | ViTForImageClassification | 29.1 M\n","1 | transform       | DataAugmentation          | 0     \n","2 | f1_score_metric | MulticlassF1Score         | 0     \n","--------------------------------------------------------------\n","29.1 M    Trainable params\n","0         Non-trainable params\n","29.1 M    Total params\n","116.395   Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: 0it [00:00, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["/Users/andry/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["/Users/andry/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["                                                                           "]},{"name":"stderr","output_type":"stream","text":["/Users/andry/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n","  rank_zero_warn(\n"]},{"name":"stdout","output_type":"stream","text":["                                    [12:14<00:00,  3.76it/s, v_num=1, training_loss_step=0.0308, training_accuracy_step=1.000, training_f1_step=1.000]\r"]},{"ename":"RuntimeError","evalue":"MPS backend out of memory (MPS allocated: 801.91 MB, other allocations: 8.28 GB, max allowed: 9.07 GB). Tried to allocate 9.23 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb Cella 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# Instantiate trainer without checkpoint\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInstantiating trainer without checkpoint...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mfit(model)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    533\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    534\u001b[0m )\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1024\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:355\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(combined_loader)\n\u001b[1;32m    354\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:134\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/loops/training_epoch_loop.py:249\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m should_check_val:\n\u001b[1;32m    248\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mvalidating \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    250\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/loops/utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/loops/evaluation_loop.py:376\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    375\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 376\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    378\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    380\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:294\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 294\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    296\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    297\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/lightning/pytorch/strategies/strategy.py:393\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mval_step_context():\n\u001b[1;32m    392\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 393\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mvalidation_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[1;32m/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb Cella 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     loss, accuracy, f1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommon_step(batch, batch_idx)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m, loss, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog(\u001b[39m\"\u001b[39m\u001b[39mtest_accuracy\u001b[39m\u001b[39m\"\u001b[39m, accuracy, on_epoch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, prog_bar\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[1;32m/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb Cella 18\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcommon_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     pixel_values, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(pixel_values)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m     criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(logits, labels)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32m/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb Cella 18\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, pixel_values):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvit(pixel_values\u001b[39m=\u001b[39;49mpixel_values)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andry/Documents/GitHub/lus-dl-framework/notebooks/experiment_nb_script.ipynb#X36sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\u001b[39m.\u001b[39mlogits\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:804\u001b[0m, in \u001b[0;36mViTForImageClassification.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    797\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[39m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    802\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 804\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvit(\n\u001b[1;32m    805\u001b[0m     pixel_values,\n\u001b[1;32m    806\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    807\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    808\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    809\u001b[0m     interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding,\n\u001b[1;32m    810\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    811\u001b[0m )\n\u001b[1;32m    813\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    815\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output[:, \u001b[39m0\u001b[39m, :])\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:587\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    581\u001b[0m     pixel_values \u001b[39m=\u001b[39m pixel_values\u001b[39m.\u001b[39mto(expected_dtype)\n\u001b[1;32m    583\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    584\u001b[0m     pixel_values, bool_masked_pos\u001b[39m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[39m=\u001b[39minterpolate_pos_encoding\n\u001b[1;32m    585\u001b[0m )\n\u001b[0;32m--> 587\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    588\u001b[0m     embedding_output,\n\u001b[1;32m    589\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    590\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    591\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    592\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    593\u001b[0m )\n\u001b[1;32m    594\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    595\u001b[0m sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm(sequence_output)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:413\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    407\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    408\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    409\u001b[0m         hidden_states,\n\u001b[1;32m    410\u001b[0m         layer_head_mask,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 413\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(hidden_states, layer_head_mask, output_attentions)\n\u001b[1;32m    415\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    417\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:365\u001b[0m, in \u001b[0;36mViTLayer.forward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39m# in ViT, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[1;32m    364\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayernorm_after(hidden_states)\n\u001b[0;32m--> 365\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(layer_output)\n\u001b[1;32m    367\u001b[0m \u001b[39m# second residual connection is done here\u001b[39;00m\n\u001b[1;32m    368\u001b[0m layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(layer_output, hidden_states)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/transformers/models/vit/modeling_vit.py:313\u001b[0m, in \u001b[0;36mViTIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    312\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 313\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate_act_fn(hidden_states)\n\u001b[1;32m    315\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/lus_dl/lib/python3.11/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39minput\u001b[39;49m)\n","\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 801.91 MB, other allocations: 8.28 GB, max allowed: 9.07 GB). Tried to allocate 9.23 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."]}],"source":["# Checkpointing\n","# Checkpoints directory\n","\n","checkpoint_path = ''\n","\n","# Check if checkpoint path is provided\n","if checkpoint_path:\n","    print(f\"Loading checkpoint from PATH: '{checkpoint_path}'...\\n\")\n","    trainer.fit(model, ckpt_path=checkpoint_path)\n","else:\n","    # Instantiate trainer without checkpoint\n","    print(\"Instantiating trainer without checkpoint...\")\n","    trainer.fit(model)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/plain":["ERROR: Failed to launch TensorBoard (exited with 1).\n","Contents of stderr:\n","TensorFlow installation not found - running with reduced feature set.\n","Address already in use\n","Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."]},"metadata":{},"output_type":"display_data"},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mSi è verificato un arresto anomalo del kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. Esaminare il codice nelle celle per identificare una possibile causa dell'errore. Per altre informazioni, fare clic su <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a>. Per altri dettagli, vedere Jupyter <a href='command:jupyter.viewOutput'>log</a>."]}],"source":["%load_ext tensorboard\n","%tensorboard --logdir tb_logs/"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
