{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\andri\\\\Progetti\\\\lus-dl-framework'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_dir = \"../.\"\n",
    "dataset_h5_path = r\"C:\\Users\\andri\\Progetti\\lus-dl-framework\\data\\iclus\\dataset.h5\"\n",
    "hospitaldict_path = r\"C:\\Users\\andri\\Progetti\\lus-dl-framework\\data\\iclus\\hospitals-patients-dict.pkl\"\n",
    "# dataset_h5_path = \"/Users/andry/Documents/GitHub/lus-dl-framework/data/iclus/dataset.h5\"\n",
    "# hospitaldict_path = \"/Users/andry/Documents/GitHub/lus-dl-framework/data/iclus/hospitals-patients-dict.pkl\"\n",
    "libraries_dir = working_dir + \"/libraries\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import pickle\n",
    "import sys\n",
    "import timm\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from kornia import tensor_to_image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from lightning.pytorch.callbacks import EarlyStopping, DeviceStatsMonitor, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch import Trainer\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "sys.path.append(working_dir)\n",
    "from lightning_modules.LUSModelLightningModule import LUSModelLightningModule\n",
    "from lightning_modules.LUSDataModule import LUSDataModule\n",
    "\n",
    "os.chdir(working_dir)\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, Subset\n",
    "import os\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from torchvision.transforms import v2\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import kornia.augmentation as K\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                                  HDF5Dataset                                 #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.index_map_path = os.path.dirname(file_path) + \"/index_map_\" + os.path.splitext(os.path.basename(file_path))[0] + \".pkl\"\n",
    "        self.h5file = h5py.File(file_path, 'r')\n",
    "        self.group_names = list(self.h5file.keys())\n",
    "        self.total_videos = sum(len(self.h5file[group_name]) for group_name in self.group_names)\n",
    "        self.check_for_index_map()\n",
    "        #self.total_frames, self.frame_index_map = self.calculate_total_frames_and_index_map()\n",
    "\n",
    "        print(f\"\\n{self.total_videos} videos ({self.total_frames} frames) loaded.\")\n",
    "\n",
    "\n",
    "    def check_for_index_map(self):\n",
    "      \"\"\"\n",
    "      Check if the index map file exists and load it if found. \n",
    "      If not found, calculate the index map and save it to a pickle file.\n",
    "\n",
    "      Parameters:\n",
    "          None\n",
    "\n",
    "      Returns:\n",
    "          None\n",
    "      \"\"\"        \n",
    "      try:\n",
    "          with open(self.index_map_path, 'rb') as f:\n",
    "              print(\"Serialized frame index map FOUND.\\n\")\n",
    "              saved_data = pickle.load(f)\n",
    "              self.total_frames = saved_data['total_frames']\n",
    "              self.frame_index_map = saved_data['frame_index_map']\n",
    "              print(\"Loaded serialized data.\\n\")\n",
    "      except FileNotFoundError:\n",
    "          print(\"Serialized frame index map NOT FOUND\\n\")\n",
    "          self.total_frames, self.frame_index_map = self.calculate_total_frames_and_index_map()\n",
    "          # Save calculated data to a pickle file\n",
    "          with open(self.index_map_path, 'wb') as f:\n",
    "              saved_data = {'total_frames': self.total_frames, 'frame_index_map': self.frame_index_map}\n",
    "              pickle.dump(saved_data, f)\n",
    "          print(\"\\nIndex map calculated and saved\")\n",
    "\n",
    "    def calculate_total_frames_and_index_map(self):\n",
    "        \"\"\"\n",
    "        Calculates the total number of frames and creates an index map for each frame.\n",
    "\n",
    "        Returns:\n",
    "            total_frames (int): The total number of frames.\n",
    "            frame_index_map (dict): A dictionary mapping frame indices to their corresponding group and video names.\n",
    "        \"\"\"\n",
    "        max_frame_idx_end = 0\n",
    "        frame_index_map = {}\n",
    "\n",
    "        # Create tqdm progress bar\n",
    "        with tqdm(total=self.total_videos, desc=\"Calculating frames and index map\", unit='video', dynamic_ncols=True) as pbar:\n",
    "            for group_name in self.group_names:\n",
    "                for video_name in self.h5file[group_name]:\n",
    "                    video_group = self.h5file[group_name][video_name]\n",
    "                    frame_idx_start = video_group.attrs['frame_idx_start']\n",
    "                    frame_idx_end = video_group.attrs['frame_idx_end']\n",
    "                    max_frame_idx_end = max(max_frame_idx_end, frame_idx_end)\n",
    "                    for i in range(frame_idx_start, frame_idx_end + 1):\n",
    "                        frame_index_map[i] = (group_name, video_name)\n",
    "                    pbar.update(1)  # Update progress bar for each video\n",
    "\n",
    "        total_frames = max_frame_idx_end + 1\n",
    "\n",
    "        return total_frames, frame_index_map\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of frames in the dataset.\n",
    "        \"\"\"\n",
    "        return self.total_frames\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Retrieves the data for a specific frame at the given index.\n",
    "    \n",
    "        Args:\n",
    "            index (int): The index of the frame to retrieve.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A tuple containing the index, frame data, target data, patient, and medical center.\n",
    "        Raises:\n",
    "            IndexError: If the index is out of range.\n",
    "        \"\"\"\n",
    "        if index < 0 or index >= self.total_frames:\n",
    "            raise IndexError(\"Index out of range\")\n",
    "    \n",
    "        group_name, video_name = self.frame_index_map[index]\n",
    "        video_group = self.h5file[group_name][video_name]\n",
    "        frame_data = video_group['frames'][f'frame_{index}'][:]\n",
    "        target_data = video_group['targets'][f'target_{index}']\n",
    "    \n",
    "        # Get metadata\n",
    "        patient = video_group.attrs['patient']\n",
    "        medical_center = video_group.attrs['medical_center']\n",
    "    \n",
    "        return index, frame_data, target_data, patient, medical_center\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                              FrameTargetDataset                              #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# Custom replica class of the dataset to train the neural network (return -> [frame,target])\n",
    "class FrameTargetDataset(Dataset):\n",
    "    def __init__(self, hdf5_dataset, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            hdf5_dataset (h5py.Dataset): The HDF5 dataset.\n",
    "        \"\"\"\n",
    "        self.hdf5_dataset = hdf5_dataset\n",
    "        self.transform = transform\n",
    "        self.resize_size = (224, 224)\n",
    "        # self.resize_size = (256, 256)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.hdf5_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the item.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the frame tensor and the target data.\n",
    "        \"\"\"\n",
    "        _, frame_data, target_data, _, _ = self.hdf5_dataset[index]\n",
    "\n",
    "        # frame_tensor = self.pp_frames(frame_data)\n",
    "        image_mean = [0.485, 0.456, 0.406]\n",
    "        image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "        frame_tensor = v2.ToTensor()(frame_data)\n",
    "        frame_tensor = v2.Resize(self.resize_size)(frame_tensor)\n",
    "        # frame_tensor = v2.Normalize(mean=image_mean, std=image_std)(frame_tensor)\n",
    "        # frame_tensor = frame_tensor.float() / 255.0\n",
    "        frame_tensor = frame_tensor.permute(0, 1, 2)\n",
    "            \n",
    "        # Target data to integer scores\n",
    "        # target_data = torch.tensor(sum(target_data))\n",
    "        target_data = int(target_data[()])\n",
    "\n",
    "        \n",
    "\n",
    "        return frame_tensor, target_data\n",
    "    \n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    # def pp_frames(self, frame_data):\n",
    "    #     \"\"\"\n",
    "    #     Preprocess the frame data.\n",
    "\n",
    "    #     Args:\n",
    "    #         frame_data: The frame data.\n",
    "\n",
    "    #     Returns:\n",
    "    #         torch.Tensor: The preprocessed frame tensor.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     size = (224, 224)\n",
    "    #     image_mean = [0.485, 0.456, 0.406]\n",
    "    #     image_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    #     frame_tensor = v2.ToTensor()(frame_data)\n",
    "    #     frame_tensor = v2.Resize(size)(frame_tensor)\n",
    "    #     frame_tensor = v2.Normalize(mean=image_mean, std=image_std)(frame_tensor)\n",
    "\n",
    "    #     return \n",
    "      \n",
    "      \n",
    "# ---------------------------------------------------------------------------- #\n",
    "#                               DataAugmentation                               #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "class DataAugmentation(nn.Module):\n",
    "    \"\"\"Module to perform data augmentation using Kornia on torch tensors.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.transforms = torch.nn.Sequential(\n",
    "        #     K.RandomRotation(degrees=(-20, 20)),  # random rotation between -20 to 20 degrees\n",
    "        #     K.RandomAffine(degrees=(-10, 10), scale=(0.8, 1.2))  # random affine transformation with rotation between -10 to 10 degrees and scale between 0.8 to 1.2\n",
    "        # )\n",
    "        \n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            K.RandomAffine(degrees=(-23, 23), scale=(1.1, 1.25), p=0.5),\n",
    "            K.RandomElasticTransform(alpha=(0.01,0.01), sigma=(0.01,0.01), p=0.3),\n",
    "            K.RandomResizedCrop(size=(224,224), scale=(0.7, 1.0), p=0.3),\n",
    "            K.RandomContrast(contrast=(0.5, 1), p=0.5),\n",
    "            K.RandomGaussianBlur((3, 3), (0.5, 1.5), p=0.3)\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()  # disable gradients for efficiency\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform data augmentation on input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape BxCxHxW.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Augmented tensor of shape BxCxHxW.\n",
    "        \"\"\"\n",
    "        x_out = self.transforms(x)\n",
    "        return x_out\n",
    "    \n",
    "def _load_dsdata_pickle(dataset, pkl_file):\n",
    "    # Check if the pickle file exists\n",
    "        if pkl_file and os.path.exists(pkl_file):\n",
    "            # If the pickle file exists, load the data from it\n",
    "            with open(pkl_file, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                medical_center_patients = data['medical_center_patients']\n",
    "                data_index = data['data_index']\n",
    "                data_map_idxs_pcm = data['data_map_idxs_pcm']\n",
    "                score_counts = data['score_counts']\n",
    "                labels = data['labels']\n",
    "        else:\n",
    "            # If the pickle file doesn't exist, create the data\n",
    "            medical_center_patients = defaultdict(set)\n",
    "            data_index = {}\n",
    "            data_map_idxs_pcm = defaultdict(list)\n",
    "            score_counts = defaultdict(int)\n",
    "            labels = []  # List to store target labels\n",
    "\n",
    "            for index, (_, _, target_data, patient, medical_center) in enumerate(tqdm(dataset)):\n",
    "                medical_center_patients[medical_center].add(patient)\n",
    "                data_index[index] = (patient, medical_center)\n",
    "                data_map_idxs_pcm[(patient, medical_center)].append(index)\n",
    "                score_counts[int(target_data[()])] += 1\n",
    "                labels.append(int(target_data[()]))\n",
    "            \n",
    "            # Save the data to a pickle file if pkl_file is provided\n",
    "            if pkl_file:\n",
    "                data = {\n",
    "                    'medical_center_patients': medical_center_patients,\n",
    "                    'data_index': data_index,\n",
    "                    'data_map_idxs_pcm': data_map_idxs_pcm,\n",
    "                    'score_counts': score_counts,\n",
    "                    'labels': labels\n",
    "                }\n",
    "                \n",
    "                with open(pkl_file, 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "        \n",
    "        return medical_center_patients, data_index, data_map_idxs_pcm, score_counts, labels\n",
    "\n",
    "def reduce_sets(seed, train, val=[], test=[], perc=1.0):\n",
    "    random.seed(seed)\n",
    "# Compute length of subsets\n",
    "    num_train_samples = int(len(train) * perc)\n",
    "    num_test_samples = int(len(test) * perc)\n",
    "\n",
    "    # Create random subsets\n",
    "    train_indices = random.sample(range(len(train)), num_train_samples)\n",
    "    test_indices = random.sample(range(len(test)), num_test_samples)\n",
    "    \n",
    "    if val:\n",
    "        num_val_samples = int(len(val) * perc)\n",
    "        val_indices = random.sample(range(len(val)), num_val_samples)\n",
    "        print(f\"dataset reduction: {int(perc*100)}% (train={len(train_indices)}, val={len(val_indices)}, test={len(test_indices)})\")\n",
    "        return train_indices, val_indices, test_indices\n",
    "    \n",
    "    print(f\"dataset reduction: {int(perc*100)}% (train={len(train_indices)}, test={len(test_indices)})\")\n",
    "    return train_indices, test_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_default_dict():\n",
    "    return defaultdict(float)\n",
    "def initialize_inner_defaultdict():\n",
    "    return defaultdict(int)\n",
    "\n",
    "def split_dataset(rseed, dataset, pkl_file, ratios=[0.6, 0.2, 0.2]):\n",
    "        \"\"\"\n",
    "        Split the dataset into training and test subsets based on a given pickle file.\n",
    "\n",
    "        Parameters:\n",
    "            pkl_file (str): The path to the pickle file.\n",
    "            rseed (int): The seed for random number generation.\n",
    "            train_ratio (float, optional): The ratio of data to be assigned to the training subset. Defaults to 0.7.\n",
    "\n",
    "        Returns:\n",
    "            train_dataset_subset (Subset): The training subset of the dataset.\n",
    "            test_dataset_subset (Subset): The test subset of the dataset.\n",
    "            split_info (dict): A dictionary containing various statistics and information about the split.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the pickle file does not exist.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        split_info_filename = os.path.dirname(pkl_file) + f\"/_split_info_{ratios[0]}.pkl\"\n",
    "        train_indices_filename = os.path.dirname(pkl_file) + f\"/_train_indices_{ratios[0]}.pkl\"\n",
    "        val_indices_filename = os.path.dirname(pkl_file) + f\"/_val_indices_{ratios[1]}.pkl\"\n",
    "        test_indices_filename = os.path.dirname(pkl_file) + f\"/_test_indices_{ratios[2]}.pkl\"\n",
    "\n",
    "        if os.path.exists(split_info_filename) and os.path.exists(train_indices_filename) and os.path.exists(val_indices_filename) and os.path.exists(test_indices_filename):\n",
    "            print(\"\\nSerialized splits found, loading ...\\n\")\n",
    "            # Load existing split data\n",
    "            with open(split_info_filename, 'rb') as split_info_file:\n",
    "                split_info = pickle.load(split_info_file)\n",
    "            with open(train_indices_filename, 'rb') as train_indices_file:\n",
    "                train_indices = pickle.load(train_indices_file)\n",
    "            with open(val_indices_filename, 'rb') as val_indices_file:\n",
    "                val_indices = pickle.load(val_indices_file)\n",
    "            with open(test_indices_filename, 'rb') as test_indices_file:\n",
    "                test_indices = pickle.load(test_indices_file)\n",
    "            return train_indices, val_indices, test_indices, split_info\n",
    "        random.seed(rseed)\n",
    "        \n",
    "        if len(ratios) == 2:\n",
    "            train_ratio, _ = ratios\n",
    "            val_ratio = 0.0\n",
    "        elif len(ratios) == 3:\n",
    "            train_ratio, val_ratio, _ = ratios\n",
    "        else:\n",
    "            raise ValueError(\"Ratios list must have 1, 2, or 3 values that sum to 1.0\")\n",
    "        \n",
    "        # 0. Gather the metadata\n",
    "        medical_center_patients, data_index, data_map_idxs_pcm, score_counts, labels = _load_dsdata_pickle(dataset, pkl_file)\n",
    "\n",
    "        # 1. Calculate the number of patients and frames for each medical center\n",
    "        frames_by_center = defaultdict(int)\n",
    "        frames_by_center_patient = defaultdict(initialize_inner_defaultdict)\n",
    "\n",
    "        for (patient, center) in data_index.values():\n",
    "            frames_by_center[center] += 1\n",
    "            frames_by_center_patient[center][patient] += 1\n",
    "        \n",
    "        # 2. Calculate the target number of frames for each split\n",
    "        total_frames = sum(frames_by_center.values())\n",
    "        train_frames = int(total_frames * train_ratio)\n",
    "        val_frames = int(total_frames * val_ratio)\n",
    "        test_frames = total_frames - train_frames - val_frames\n",
    "\n",
    "        # 3. Create a dictionary to track patient percentages for each center\n",
    "        patient_perc_by_center = defaultdict(create_default_dict)\n",
    "        for center, patients in medical_center_patients.items():\n",
    "            patients = list(patients)\n",
    "\n",
    "            for patient in patients:\n",
    "                patient_frames = frames_by_center_patient[center][patient]\n",
    "                patient_percentage = patient_frames / total_frames\n",
    "                patient_perc_by_center[center][patient] = patient_percentage\n",
    "        \n",
    "        # 4. Splitting the dataset by patients taking into account frames ratio\n",
    "        # lists\n",
    "        train_indices = []\n",
    "        val_indices = []\n",
    "        test_indices = []\n",
    "\n",
    "        # sets to store statistics about medical centers and patients\n",
    "        train_patients_by_center = defaultdict(set)\n",
    "        val_patients_by_center = defaultdict(set)\n",
    "        test_patients_by_center = defaultdict(set)\n",
    "\n",
    "        # 4.1 Test set\n",
    "        while len(test_indices) < test_frames:\n",
    "            center = random.choice(list(patient_perc_by_center.keys()))\n",
    "            patients = list(patient_perc_by_center[center].keys())\n",
    "            if patients:\n",
    "                patient = random.choice(patients)\n",
    "                if center in patient_perc_by_center and patient in patient_perc_by_center[center]:\n",
    "                    if len(test_indices) + patient_perc_by_center[center][patient] * total_frames <= test_frames:\n",
    "                        test_indices.extend(data_map_idxs_pcm[(patient, center)])\n",
    "                        test_patients_by_center[center].add(patient)\n",
    "                        del patient_perc_by_center[center][patient]\n",
    "                    else:\n",
    "                        # Se supera test_frames, cerca i pazienti rimasti che possono essere aggiunti per avvicinare il rapporto\n",
    "                        remaining_frames = test_frames - len(test_indices)\n",
    "                        candidates = [p for p in patients if patient_perc_by_center[center][p] * total_frames <= remaining_frames]\n",
    "                        if candidates:\n",
    "                            # Ordina i candidati in base a quanto si avvicinano al rapporto desiderato\n",
    "                            candidates = sorted(candidates, key=lambda p: abs((len(test_indices) + patient_perc_by_center[center][p] * total_frames) / test_frames - 1))\n",
    "                            \n",
    "                            for best_candidate in candidates:\n",
    "                                if len(test_indices) + patient_perc_by_center[center][best_candidate] * total_frames <= test_frames:\n",
    "                                    test_indices.extend(data_map_idxs_pcm[(best_candidate, center)])\n",
    "                                    test_patients_by_center[center].add(best_candidate)\n",
    "                                    del patient_perc_by_center[center][best_candidate]\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "        # 4.2 Validation set\n",
    "        while len(val_indices) < val_frames:\n",
    "            center = random.choice(list(patient_perc_by_center.keys()))\n",
    "            patients = list(patient_perc_by_center[center].keys())\n",
    "            if patients:\n",
    "                patient = random.choice(patients)\n",
    "                if center in patient_perc_by_center and patient in patient_perc_by_center[center]:\n",
    "                    if len(val_indices) + patient_perc_by_center[center][patient] * total_frames <= val_frames:\n",
    "                        val_indices.extend(data_map_idxs_pcm[(patient, center)])\n",
    "                        val_patients_by_center[center].add(patient)\n",
    "                        del patient_perc_by_center[center][patient]\n",
    "                    else:\n",
    "                        # Se supera train_frames, cerca i pazienti rimasti che possono essere aggiunti per avvicinare il rapporto\n",
    "                        remaining_frames = val_frames - len(val_indices)\n",
    "                        candidates = [p for p in patients if patient_perc_by_center[center][p] * total_frames <= remaining_frames]\n",
    "                        if candidates:\n",
    "                            # Ordina i candidati in base a quanto si avvicinano al rapporto desiderato\n",
    "                            candidates = sorted(candidates, key=lambda p: abs((len(val_indices) + patient_perc_by_center[center][p] * total_frames) / val_frames - 1))\n",
    "                            \n",
    "                            for best_candidate in candidates:\n",
    "                                if len(val_indices) + patient_perc_by_center[center][best_candidate] * total_frames <= val_frames:\n",
    "                                    val_indices.extend(data_map_idxs_pcm[(best_candidate, center)])\n",
    "                                    val_patients_by_center[center].add(best_candidate)\n",
    "                                    del patient_perc_by_center[center][best_candidate]\n",
    "                        else:\n",
    "                            break\n",
    "        \n",
    "        # 4.3 Train set\n",
    "        for center in patient_perc_by_center:\n",
    "            for patient in patient_perc_by_center[center]:\n",
    "                train_indices.extend(data_map_idxs_pcm[(patient, center)])\n",
    "                train_patients_by_center[center].add(patient)\n",
    "        \n",
    "        # 5. Diagnostic checks and return values\n",
    "        total_frames_calc = len(train_indices) + len(val_indices) + len(test_indices)\n",
    "        if total_frames != total_frames_calc:\n",
    "            print(f\"dataset splitting gone wrong (expected: {total_frames}, got:{total_frames_calc})\")\n",
    "        \n",
    "        # Sum up statistics info\n",
    "        split_info = {\n",
    "            'medical_center_patients': medical_center_patients,\n",
    "            'frames_by_center': frames_by_center,\n",
    "            'train_patients_by_center': train_patients_by_center,\n",
    "            'val_patients_by_center': val_patients_by_center,\n",
    "            'test_patients_by_center': test_patients_by_center,\n",
    "            'frames_by_center_patient': frames_by_center_patient,\n",
    "            'score_counts': score_counts,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "        train_idxs_p = round((len(train_indices) / len(dataset)) * 100)\n",
    "        val_idxs_p = round((len(val_indices) / len(dataset)) * 100)\n",
    "        test_idxs_p = 100 - (train_idxs_p + val_idxs_p)\n",
    "\n",
    "        if val_ratio == 0.0:\n",
    "            print(f\"dataset split: train={len(train_indices)}({train_idxs_p}%), test={len(test_indices)}({test_idxs_p}%)\")\n",
    "            return train_indices, test_indices, split_info\n",
    "        \n",
    "        print(f\"dataset split: train={len(train_indices)}({train_idxs_p}%), val={len(val_indices)}({val_idxs_p}%), test={len(test_indices)}({test_idxs_p}%)\")\n",
    "\n",
    "        \n",
    "        # Serialize the split data for future use\n",
    "        print(f\"\\nSerializing splits...\\n\") \n",
    "        with open(split_info_filename, 'wb') as split_info_file:\n",
    "            pickle.dump(split_info, split_info_file)\n",
    "        with open(train_indices_filename, 'wb') as train_indices_file:\n",
    "            pickle.dump(train_indices, train_indices_file)\n",
    "        with open(val_indices_filename, 'wb') as val_indices_file:\n",
    "            pickle.dump(val_indices, val_indices_file)\n",
    "        with open(test_indices_filename, 'wb') as test_indices_file:\n",
    "            pickle.dump(test_indices, test_indices_file)\n",
    "        return train_indices, val_indices, test_indices, split_info   \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized frame index map FOUND.\n",
      "\n",
      "Loaded serialized data.\n",
      "\n",
      "\n",
      "277 videos (58924 frames) loaded.\n",
      "Serialized frame index map FOUND.\n",
      "\n",
      "Loaded serialized data.\n",
      "\n",
      "\n",
      "277 videos (58924 frames) loaded.\n",
      "Split ratios: [0.6, 0.2, 0.2]\n",
      "\n",
      "Serialized splits found, loading ...\n",
      "\n",
      "dataset reduction: 15% (train=5313, val=1757, test=1767)\n",
      "Train size: 5313\n",
      "Test size: 1767\n",
      "Validation size: 1757\n"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "batch_size = 8\n",
    "epochs = 20\n",
    "lr = 1e-2\n",
    "gamma = 0.7\n",
    "rseed = 42\n",
    "train_ratio = 0.6\n",
    "num_workers = 0\n",
    "trim_data = 0.15\n",
    "\n",
    "dataset = HDF5Dataset(dataset_h5_path)\n",
    "\n",
    "train_indices_path = os.path.ddataset = HDF5Dataset(dataset_h5_path)\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "test_indices = []\n",
    "\n",
    "train_ratio = train_ratio\n",
    "test_ratio = (1 - train_ratio)/2\n",
    "val_ratio = test_ratio\n",
    "ratios = [train_ratio, test_ratio, val_ratio]\n",
    "print(f\"Split ratios: {ratios}\")\n",
    "\n",
    "# train_indices_path = os.path.dirname(dataset_h5_path) + f\"/train_indices_{train_ratio}.pkl\"\n",
    "# test_indices_path = os.path.dirname(dataset_h5_path) + f\"/test_indices_{test_ratio}.pkl\"\n",
    "# val_indices_path = os.path.dirname(dataset_h5_path) + f\"/val_indices_{val_ratio}.pkl\"\n",
    "# split_info_path = os.path.dirname(dataset_h5_path) + f\"/split_info_{train_ratio}.pkl\"\n",
    "\n",
    "# if os.path.exists(train_indices_path) and os.path.exists(test_indices_path):\n",
    "    \n",
    "#     print(\"Loading pickled indices\")\n",
    "    \n",
    "#     with open(train_indices_path, 'rb') as train_pickle_file:\n",
    "#         train_indices = pickle.load(train_pickle_file)\n",
    "        \n",
    "#     with open(test_indices_path, 'rb') as test_pickle_file:\n",
    "#         test_indices = pickle.load(test_pickle_file)\n",
    "        \n",
    "#     with open(test_indices_path, 'rb') as val_pickle_file:\n",
    "#         val_indices = pickle.load(val_pickle_file)\n",
    "        \n",
    "# else:\n",
    "train_indices, val_indices, test_indices, split_info = split_dataset(\n",
    "    rseed=rseed,\n",
    "    dataset=dataset,\n",
    "    pkl_file=hospitaldict_path,\n",
    "    ratios=ratios)\n",
    "\n",
    "\n",
    "# Pickle the indices\n",
    "# with open(train_indices_path, 'wb') as train_pickle_file:\n",
    "#     pickle.dump(train_indices, train_pickle_file)\n",
    "    \n",
    "# with open(test_indices_path, 'wb') as test_pickle_file:\n",
    "#     pickle.dump(test_indices, test_pickle_file)\n",
    "    \n",
    "# with open(val_indices_path, 'wb') as val_pickle_file:\n",
    "#     pickle.dump(val_indices, val_pickle_file)\n",
    "    \n",
    "# Create training and test subsets\n",
    "train_subset = Subset(dataset, train_indices)\n",
    "test_subset = Subset(dataset, test_indices)  \n",
    "val_subset = Subset(dataset, val_indices)  \n",
    "\n",
    "# if trim_data: \n",
    "#     test_subset_size = train_ratio/2\n",
    "#     test_subset = Subset(test_subset, range(int(test_subset_size * len(test_indices))))\n",
    "\n",
    "if trim_data:\n",
    "    train_indices_trimmed, val_indices_trimmed, test_indices_trimmed = reduce_sets(train_subset, val_subset, test_subset, trim_data)\n",
    "\n",
    "    train_subset = Subset(dataset, train_indices_trimmed)\n",
    "    test_subset = Subset(dataset, test_indices_trimmed)  \n",
    "    val_subset = Subset(dataset, val_indices_trimmed) \n",
    "\n",
    "train_dataset = FrameTargetDataset(train_subset)\n",
    "test_dataset = FrameTargetDataset(test_subset)\n",
    "val_dataset = FrameTargetDataset(val_subset)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")    \n",
    "print(f\"Validation size: {len(val_dataset)}\")    \n",
    "\n",
    "lus_data_module = LUSDataModule(train_dataset, \n",
    "                                test_dataset,\n",
    "                                val_dataset,\n",
    "                                num_workers, \n",
    "                                batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[31691, 31692, 31693, 31694, 31695, 31696, 31697, 31698, 31699, 31700]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7296, 1639, 18024, 16049, 14628, 9144, 6717, 5697, 27651, 2082]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices_trimmed[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights:  [0.88899207 0.93380259 0.76457002 2.01484642]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "ds_labels = split_info['labels']\n",
    "\n",
    "# Extract the train and test set labels\n",
    "y_train_labels = np.array(ds_labels)[train_indices]\n",
    "y_test_labels = np.array(ds_labels)[test_indices]\n",
    "\n",
    "# Calculate class balance using 'compute_class_weight'\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
    "weights_tensor = torch.Tensor(class_weights)\n",
    "print(\"Class Weights: \", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lus_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
