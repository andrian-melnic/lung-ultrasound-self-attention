# -*- coding: utf-8 -*-
"""full-dataset-preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HGlb_KzfywoyfQCYxBy35UdbYK4Pelnn

# GDrive e Imports
"""

from google.colab import drive

# Monta Google Drive
drive.mount('/content/drive')

import os
import torch
import scipy.io as sio
from torch.utils.data import Dataset
from pathlib import Path

import numpy as np
import matplotlib.pyplot as plt

from torch.utils.data import DataLoader

from tqdm import tqdm

import h5py

from itertools import islice

"""# Dataset pre-processing

## Custom class definition
"""
base_path = '/content/drive/MyDrive/Tesi/dataset/Covid19/'  # Base path
folder_names = ['Brescia','Gemelli - Roma','Lucca','Pavia','No Covid Data']  # List of folder names relative to the base_path

dataset = LazyLoadingDataset(folder_names, base_path)

"""## Frames resolutions"""

import pickle

def get_frames_resolutions(dataset, num_videos=None):
    video_resolutions = {}
    video_patients = {}
    nframes = 0
    nframes_linear = 0
    nframes_convex = 0
    nframes_unclassified = 0
    total_videos = len(dataset)
    num_videos = num_videos if num_videos is not None else total_videos
    broken_videos = []

    with tqdm(total=num_videos, desc="Getting frame resolutions", unit='video', dynamic_ncols=True) as pbar:
        for video_data, _ in dataset:
            resolutions = set()
            num_frames = video_data.get_num_frames()
            if num_frames == -1:
                print(f"\nError: Video '{video_data.get_video_name()}' has an invalid number of frames (-1). Skipping this video.")
                broken_videos.append({
                    'video_name': video_data.get_video_name(),
                    'patient': video_data.get_patient(),
                    'center': video_data.get_medical_center()
                })
                pbar.update(1)  # Increase the count even for broken videos
                continue

            nframes += num_frames
            pbar.set_description(f"Getting frame resolutions (frames: {nframes})")
            video_name = video_data.get_video_name()
            patient = video_data.get_patient()
            center = video_data.get_medical_center()

            if 'linear' in video_name:
                nframes_linear += num_frames
            elif 'convex' in video_name:
                nframes_convex += num_frames
            else:
                nframes_unclassified += num_frames

            for i in range(num_frames):
                frame_data = video_data.get_frame(i)
                resolution = frame_data.shape[:2]
                resolutions.add(resolution)
            video_resolutions[video_name] = resolutions
            video_patients[video_name] = (patient, center)
            pbar.update(1)
            if pbar.n == num_videos:
                break

    return video_resolutions, video_patients, nframes, nframes_linear, nframes_convex, nframes_unclassified, broken_videos

# Get frame resolutions and patients for each video and the total number of frames
resolutions, patients, nframes, nframes_linear, nframes_convex, nframes_unclassified, broken_videos = get_frames_resolutions(dataset)

# Create a dictionary to store the data
pickle_resolutions_dict = {
    'resolutions': resolutions,
    'patients': patients,
    'nframes': nframes,
    'nframes_linear': nframes_linear,
    'nframes_convex': nframes_convex,
    'nframes_unclassified': nframes_unclassified,
    'broken_videos': broken_videos
}

# Save the data dictionary to a pickle file
pickle_frame_res_file = '/content/drive/MyDrive/Tesi/dataset/frames_res.pkl'
with open(pickle_frame_res_file, 'wb') as f:
    pickle.dump(pickle_resolutions_dict, f)

# Print a message to indicate that the data has been saved
print("\nResolutions data has been saved to:", pickle_frame_res_file)

# Load data from the pickle file
with open(pickle_frame_res_file, 'rb') as f:
    data_dict = pickle.load(f)

# Access the data as needed
pckl_resolutions = data_dict['resolutions']
pckl_patients = data_dict['patients']
pckl_nframes = data_dict['nframes']
pckl_nframes_linear = data_dict['nframes_linear']
pckl_nframes_convex = data_dict['nframes_convex']
pckl_nframes_unclassified = data_dict['nframes_unclassified']
pckl_broken_videos = data_dict['broken_videos']

total_frames_from_doc = 58924

# Create an empty set to track encountered video names
encountered_video_names = set()

# Verify if frames have consistent resolutions within each video group
varying_resolutions_count = 0
for video_name, resolutions_set in pckl_resolutions.items():
    if len(resolutions_set) == 1:
        print(f"Video '{video_name}' has consistent resolution: {resolutions_set.pop()}")
    else:
        varying_resolutions_count += 1
        print(f"Video '{video_name}' has varying resolutions: {resolutions_set}")

# Verify if patients have videos with consistent resolutions within each center of research
patient_resolutions = {}
varying_patient_resolutions_count = 0
for video_name, (patient, center) in pckl_patients.items():
    resolutions_set = pckl_resolutions[video_name]
    patient_id = f"{patient}_{center}"
    if patient_id not in patient_resolutions:
        patient_resolutions[patient_id] = resolutions_set
    else:
        if patient_resolutions[patient_id] != resolutions_set:
            varying_patient_resolutions_count += 1
            print(f"Patient '{patient}' at '{center}' has varying resolutions across videos.")

# Print the total number of frames, frames with linear, convex, and unclassified probes, and broken videos at the end of the execution
print(f"\nVideo: {len(dataset)}")
print(f"Frames with linear probes: {pckl_nframes_linear}")
print(f"Frames with convex probes: {pckl_nframes_convex}")
print(f"Frames unclassified: {pckl_nframes_unclassified}")
print(f"Broken videos: {len(pckl_broken_videos)} ({total_frames_from_doc - pckl_nframes} frames)")
print(f"Total frames in the dataset: {pckl_nframes}")

# Print the count of videos with varying resolutions and varying patient resolutions
print(f"Videos with varying resolutions: {varying_resolutions_count}")
print(f"Patients with varying resolutions across videos: {varying_patient_resolutions_count}")

"""## Custom classes workflow test"""

import numpy as np
import matplotlib.pyplot as plt

# Function to display and print frame information
def print_frame(frame_idx, dataset_idx):
    if dataset_idx < 0 or dataset_idx >= len(dataset):
        print(f"Error: Invalid dataset index {dataset_idx}. Dataset index must be between 0 and {len(dataset) - 1}.")
        dataset_idx = len(dataset) - 1

    video_data, target_data = dataset[dataset_idx]
    num_frames = video_data.get_num_frames()

    if frame_idx < 0 or frame_idx >= num_frames:
        print(f"Error: Invalid frame index {frame_idx}. Frame index must be between 0 and {num_frames - 1}.")
        frame_idx = num_frames - 1

    frame = video_data.get_frame(frame_idx)
    score = target_data.get_score(frame_idx)
    video_name = video_data.get_video_name()
    patient = video_data.get_patient()
    medical_center = video_data.get_medical_center()

    # Create an image from RGB
    img = np.zeros_like(frame, dtype=np.uint8)
    img[:, :, 0] = frame[:, :, 0]  # Red channel
    img[:, :, 1] = frame[:, :, 1]  # Green channel
    img[:, :, 2] = frame[:, :, 2]  # Blue channel

    # Show the new image
    plt.imshow(img)
    plt.axis('off')

    # Add annotations to the image
    plt.annotate(f"Score: {score}", (20, img.shape[0] - 20), color='white', fontsize=10, ha='left', va='bottom')
    plt.annotate(f"Frame shape: {frame.shape}", (20, img.shape[0] - 45), color='white', fontsize=10, ha='left', va='bottom')
    plt.annotate(f"Patient: {patient}", (20, img.shape[0] - 70), color='white', fontsize=10, ha='left', va='bottom')
    plt.annotate(f"Medical center: {medical_center}", (20, img.shape[0] - 95), color='white', fontsize=10, ha='left', va='bottom')
    plt.annotate(f"Video name: {video_name}", (20, img.shape[0] - 120), color='white', fontsize=10, ha='left', va='bottom')

    plt.title(f"Frame {frame_idx}, Dataset {dataset_idx}")
    plt.show()

frame_idx = 52
dataset_idx = 2

print_frame(frame_idx, dataset_idx)

"""# Dataset conversion to h5"""

# Function to save a single video data to the HDF5 file
def save_video_data(h5file, group_name, video_name, video_data, target_data, patient_reference, medical_center, start_index):
    num_frames = video_data.get_num_frames()
    if num_frames == -1:
        print(f"\rError: Video '{medical_center}/{patient_reference}/{video_name}' has an invalid number of frames (-1). Skipping this video.")
        return start_index

    group = h5file.require_group(group_name)

    # Check if the video_name already exists in the group
    count = 2
    new_video_name = video_name
    while new_video_name in group:
        new_video_name = f"{video_name}_{count}"
        count += 1

    # If the video_name is changed, print a warning message
    if new_video_name != video_name:
        print(f"\rWarning: Video group '{medical_center}/{patient_reference}/{video_name}' already exists. Renaming the new video to '{new_video_name}'.")

    video_group = group.require_group(new_video_name)

    frames_group = video_group.require_group('frames')
    targets_group = video_group.require_group('targets')

    # Add patient_reference and medical_center attributes to the video_group
    video_group.attrs['patient_reference'] = patient_reference
    video_group.attrs['medical_center'] = medical_center

    for i in range(num_frames):
        frame_data = video_data.get_frame(i)
        frames_group.create_dataset(f'frame_{start_index + i}', data=frame_data, compression='gzip')

        target_data_i = target_data.get_score(i)
        targets_group.create_dataset(f'target_{start_index + i}', data=target_data_i, compression='gzip')

    # Update the 'idx_start' and 'idx_end' attributes
    video_group.attrs['frame_idx_start'] = start_index
    video_group.attrs['frame_idx_end'] = start_index + num_frames - 1

    return start_index + num_frames

# Create the HDF5 file and save the dataset
def convert_dataset_to_h5(dataset, output_file, num_videos=None):
    with h5py.File(output_file, 'w') as h5file:
        convex_group = h5file.create_group('convex')
        linear_group = h5file.create_group('linear')

        current_index_convex = 0
        current_index_linear = 0

        dataset_subset = islice(dataset, num_videos) if num_videos is not None else dataset

        with tqdm(total=num_videos if num_videos is not None else len(dataset), desc="Converting dataset to HDF5", dynamic_ncols=True, unit="video") as pbar_outer:
            for video_data, target_data in dataset_subset:
                video_name = video_data.get_video_name()
                patient = video_data.get_patient()
                medical_center = video_data.get_medical_center()

                if 'convex' in video_name:
                    current_index_convex = save_video_data(h5file, 'convex', video_name, video_data, target_data, patient, medical_center, current_index_convex)
                elif 'linear' in video_name:
                    current_index_linear = save_video_data(h5file, 'linear', video_name, video_data, target_data, patient, medical_center, current_index_linear)
                else:
                    print(f"\rWarning: Video '{medical_center}/{patient}/{video_name}' has an unclassified probe. Skipping this video.")

                pbar_outer.update(1)

                # Monitor file size
                file_size_gb = os.path.getsize(output_file) / (1024.0 ** 3)  # Convert to GB
                pbar_outer.set_postfix(file_size=f"{file_size_gb:.2f} GB")

# Path of the HDF5 file where the data will be saved
output_file = '/content/drive/MyDrive/Tesi/dataset/dataset_full.h5'

# Check the existence of the HDF5 file and delete it if it exists
if os.path.exists(output_file):
    os.remove(output_file)

# Launch the conversion
convert_dataset_to_h5(dataset, output_file)

"""# Testing

## Exploring the h5 dataset
"""

import h5py

# Function to print diagnostic information about the dataset
def print_dataset_info(h5file):
    for group_name in h5file:
        group = h5file[group_name]
        print(f"Group: {group_name}")

        num_frames_counter = 0

        for video_name in group:
            video_group = group[video_name]
            print(f"  Video: {video_name}")

            frames_group = video_group['frames']
            num_frames = len(frames_group)
            print(f"    Number of frames: {num_frames}")

            targets_group = video_group['targets']
            num_targets = len(targets_group)
            print(f"    Number of targets: {num_targets}")

            # Get patient_reference and medical_center attributes
            patient_reference = video_group.attrs['patient_reference']
            medical_center = video_group.attrs['medical_center']
            print(f"    Patient Reference: {patient_reference}")
            print(f"    Medical Center: {medical_center}")

            # Get idx attributes
            fis = video_group.attrs['frame_idx_start']
            fie = video_group.attrs['frame_idx_end']
            print(f"    Frame idx Range: {fis} - {fie}")

            continue

            print("    Frame and Target information:")
            for i in range(num_frames):
                frame_data = frames_group[f'frame_{num_frames_counter+i}']
                target_data = targets_group[f'target_{num_frames_counter+i}']
                print(f"      Frame {num_frames_counter+i}: Shape = {frame_data.shape}, Target = {target_data.shape}")

            num_frames_counter += num_frames

# Load the dataset
with h5py.File('/content/drive/MyDrive/Tesi/dataset/dataset_full.h5', 'r') as h5file:
    # Call the function to print diagnostic information about the dataset
    print_dataset_info(h5file)

"""## PyTorch DataLoader

### Version 1
"""

import h5py
import torch
from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt
from PIL import Image

import torchvision.transforms as transforms

class HDF5Dataset(Dataset):
    def __init__(self, file_path):
        self.file_path = file_path
        self.h5file = h5py.File(file_path, 'r')
        self.group_names = list(self.h5file.keys())
        self.total_frames = self.calculate_total_frames()
        #self.total_frames = 45560
        self.resize_size = (100, 150)
        #self.total_frames = sum(len(self.h5file[group_name][video_name]['frames']) for group_name in self.group_names for video_name in self.h5file[group_name])

    def calculate_total_frames(self):
        max_frame_idx_end = 0
        for group_name in self.group_names:
            for video_name in self.h5file[group_name]:
                video_group = self.h5file[group_name][video_name]
                frame_idx_end = video_group.attrs['frame_idx_end']
                max_frame_idx_end = max(max_frame_idx_end, frame_idx_end)

        print(f"{max_frame_idx_end} frames loaded.")
        return max_frame_idx_end + 1

    def __len__(self):
        return self.total_frames

    def __getitem__(self, index):
        if index < 0 or index >= self.total_frames:
            raise IndexError("Index out of range")

        for group_name in self.group_names:
            for video_name in self.h5file[group_name]:
                video_group = self.h5file[group_name][video_name]
                frame_idx_start = video_group.attrs['frame_idx_start']
                frame_idx_end = video_group.attrs['frame_idx_end']
                if frame_idx_start <= index <= frame_idx_end:
                    frame_data = video_group['frames'][f'frame_{index}'][:]
                    target_data = video_group['targets'][f'target_{index}'][:]

                    # Apply Resize transformation
                    frame_tensor = transforms.ToTensor()(frame_data)
                    frame_tensor = transforms.Resize(self.resize_size, antialias=True)(frame_tensor)
                    frame_tensor = frame_tensor.permute(1, 2, 0) # Move channels to the last dimension (needed after resize)

                    #print(f"found frame {index} in video {video_name} in group {group_name}")

                    return index, frame_tensor, target_data

        return None

# Path of the HDF5 file created earlier
input_file = '/content/drive/MyDrive/Tesi/dataset/dataset_full.h5'

# Create the custom dataset
dataset = HDF5Dataset(input_file)

# Create the DataLoader
batch_size = 128  # Choose the desired batch size
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

"""### Version 2 (optimized)"""

import h5py
import torch
from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt
from PIL import Image

import torchvision.transforms as transforms

class HDF5Dataset(Dataset):
    def __init__(self, file_path):
        self.file_path = file_path
        self.h5file = h5py.File(file_path, 'r')
        self.group_names = list(self.h5file.keys())
        self.total_videos = sum(len(self.h5file[group_name]) for group_name in self.group_names)
        self.total_frames, self.frame_index_map = self.calculate_total_frames_and_index_map()
        self.resize_size = (100, 150)

        print(f"\n{self.total_videos} videos ({self.total_frames} frames) loaded.")

    def calculate_total_frames_and_index_map(self):
        max_frame_idx_end = 0
        frame_index_map = {}

        # Create tqdm progress bar
        with tqdm(total=self.total_videos, desc="Calculating frames and index map", unit='video', dynamic_ncols=True) as pbar:
            for group_name in self.group_names:
                for video_name in self.h5file[group_name]:
                    video_group = self.h5file[group_name][video_name]
                    frame_idx_start = video_group.attrs['frame_idx_start']
                    frame_idx_end = video_group.attrs['frame_idx_end']
                    max_frame_idx_end = max(max_frame_idx_end, frame_idx_end)
                    for i in range(frame_idx_start, frame_idx_end + 1):
                        frame_index_map[i] = (group_name, video_name)
                    pbar.update(1)  # Update progress bar for each video

        total_frames = max_frame_idx_end + 1

        return total_frames, frame_index_map

    def __len__(self):
        return self.total_frames

    def __getitem__(self, index):
        if index < 0 or index >= self.total_frames:
            raise IndexError("Index out of range")

        group_name, video_name = self.frame_index_map[index]
        video_group = self.h5file[group_name][video_name]
        frame_data = video_group['frames'][f'frame_{index}'][:]
        target_data = video_group['targets'][f'target_{index}'][:]

        # Apply Resize transformation
        frame_tensor = transforms.ToTensor()(frame_data)
        frame_tensor = transforms.Resize(self.resize_size, antialias=True)(frame_tensor)
        frame_tensor = frame_tensor.permute(1, 2, 0) # Move channels to the last dimension (needed after resize)

        return index, frame_tensor, target_data

# Path of the HDF5 file created earlier
input_file = '/content/drive/MyDrive/Tesi/dataset/dataset_full.h5'

# Create the custom dataset
dataset = HDF5Dataset(input_file)

# Create the DataLoader
batch_size = 128
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

"""## Printing the frames grid cycling the batches"""

# Function to create a flexible grid for displaying frames
def create_flexible_grid(batch_size):
    num_rows = 1
    num_cols = 1
    while num_rows * num_cols < batch_size:
        if num_rows == num_cols:
            num_cols *= 2
        else:
            num_rows *= 2

    return num_rows, num_cols

def get_label_color(target_array):
    color_mapping = {
        (0, 0, 0): 'lightgreen',
        (1, 0, 0): 'gold',
        (1, 1, 0): 'orange',
        (1, 1, 1): 'red',
    }
    return color_mapping.get(tuple(target_array), 'black')

# Loop through the dataloader to load frames and targets in batches
for (idx_batch, frames_batch, targets_batch) in dataloader:
    if frames_batch is not None and targets_batch is not None:
        # Get the flexible grid size for the batch
        rows, cols = create_flexible_grid(batch_size)

        # Create a subplot grid for frames
        fig, axs = plt.subplots(rows, cols, figsize=(cols, rows))
        plt.subplots_adjust(wspace=0.1, hspace=0.1)

        for i, (frame_tensor, target_tensor) in enumerate(zip(frames_batch, targets_batch)):
            frame_array = frame_tensor.numpy()
            target_array = target_tensor.numpy()
            index = idx_batch[i].numpy()

            ax = axs[i // cols, i % cols]
            ax.imshow(frame_array)
            ax.axis('off')
            ax.set_title(index, fontsize=7, color='black', ha='center', va='center')

            label = f"Target: {target_array}"
            color = get_label_color(target_array)
            ax.text(0.5, 0.05, label, fontsize=6.5, ha='center', va='bottom', color=color, transform=ax.transAxes)

        plt.show()

        print("\n")